# Deploying Applications <img src="https://static-00.iconduck.com/assets.00/google-gke-icon-512x457-q6s0e3iu.png" alt="gke logo" width="80"/>

1. [Pod reliability with health checks](https://github.com/Nagrimor/learning-notes/blob/main/GKE%20Beginner%20to%20Pro/3.0%20Deploying%20Applications.md#pod-reliability-with-health-checks)
2. [Accessing External services](https://github.com/Nagrimor/learning-notes/blob/main/GKE%20Beginner%20to%20Pro/3.0%20Deploying%20Applications.md#accessing-external-services)
3. [Volumes and Persistant Storage](https://github.com/Nagrimor/learning-notes/blob/main/GKE%20Beginner%20to%20Pro/3.0%20Deploying%20Applications.md#volumes-and-persistent-storage)
4. [Config Maps & Secrets](https://github.com/Nagrimor/learning-notes/blob/main/GKE%20Beginner%20to%20Pro/3.0%20Deploying%20Applications.md#config-maps--secrets)
5. [Deployment Patterns](https://github.com/Nagrimor/learning-notes/blob/main/GKE%20Beginner%20to%20Pro/3.0%20Deploying%20Applications.md#deployment-patterns)
6. [Autoscaling](https://github.com/Nagrimor/learning-notes/blob/main/GKE%20Beginner%20to%20Pro/3.0%20Deploying%20Applications.md#autoscaling)

## **Pod reliability with health checks**

* **Liveness Probes**
 * A simple check performed by the kubelet service that runs on every node
 * It is something like a built-in monitoring for your **Pods**
 * It can check an HTTP endpoint, TCP socket, or run a command
 * If the Liveness Probe fails, GKE will know something is wrong with a **Pod** and will attempt to replace it
 * Example yaml file for a **Pod** with a Liveness Probe:<br>
 <img src="https://i.imgur.com/UbYl3NE.png" width="500"/><br>

* **Readiness Probe**
 * If a **Pod** usually takes longer to boot up it is better to use the Readiness Probe
 * It is similar to the Liveness Probe and uses same methods
 * It tells GKE whether a **Pod** is ready to serve traffic
 * Sometimes a **Pod** needs longer time (for example to import data) and the Readiness Probe won't direct traffic until it's ready

* You should ideally use both probes together to tell GKE when a **Pods** are ready and to keep a life on their health
 * Example yaml file with both probes:<br>
  <img src="https://i.imgur.com/EC3Ifzs.png" width="500"/><br>

* **Probe Configuration**
 * There are a few main ways to configure Probes
 * They are all defined by a handler in the yaml file
 * There are 3 supported types of handlers:
   * **ExecAction**<br>
   This can run a specific command inside the container and report on its exit code
   * **TCPSocketAction**<br>
   This checks for a response on a specific TCP port
   * **HTTPGetAction**<br>
   This checks a URL for a status code. (HTTP 200 code is ok - everything else is usually not)
* You can also define the initial delay and frequency of probes and also change the default seconds for timeout
  * **initalDelaySeconds**
  * **periodSeconds**
  * **timeoutSeconds**
* You can also modify the default numbers before a probe considers a check a success of fail
 * **successThreshold**
 * **failureThreshold**

* All **Pods** start in a **Pending state** **>>>** Then hopefully they reach a **Running state** when all of the containers inside them are ready **>>>** In addition, certain types of **Pods** are designed to execute an Action in their containers and then stop when they have entered a **Succeeded state**:<br>
<img src="https://media.giphy.com/media/KPAP9GrtfjLwqYMsdl/giphy.gif" width="500"/><br>

* If a **Pod** does not start up or exit properly a **Pod** enters a **Failed state** **>>>** If the Scheduler cannot determine the state of the **Pod** it goes to **Unknown state**<br>
<img src="https://media.giphy.com/media/9gihGDRBs0fHtS9Nfx/giphy.gif" width="500"/><br>

* To ensure the health of a **Pod** we add a **Readiness check** to tell GKE that our **Pod** is ready to serve traffic **>>>** And we add **Liveness probe** that continually monitors the health of our **Pod** **>>>** If the **Liveness probe** fails the **Pod** is considered to be in a **Failed state** and GKE will try to replace it:<br>
<img src="https://media.giphy.com/media/ZfImyvoLXiuglQFN1P/giphy.gif" width="500"/><br>

## Accessing External services

* Not everything in our application stack will live inside our GKE cluster
* To access an external service we can use:
 * **Service Endpoints**:
   * They are part of GKE built-in Service Discovery System
   * We can create a Service Endpoint by defining a service yaml file with **no Selector** and a corresponding endpoint object
   * When we reference the service, GKE finds the endpoint with the corresponding name
   * Endpoints map **external services** to Service objects using an IP address or a fully qualified domain name (fqdm)
   * Because we are using Service Discovery we can access external services via internal DNS
   * Example:<br>
   <img src="https://i.imgur.com/y8bimBB.png" width="500"/><br>
   <img src="https://i.imgur.com/90UWWTP.png" width="500"/><br>
   <img src="https://i.imgur.com/4cUDijI.png" width="500"/><br>
   * We can also point to external FQDN if there is one provided and no internal endpoint objects are required. Note that we also get our internal DNS name to use with **Pods** in this case as well:<br>
   <img src="https://i.imgur.com/b01Hdai.png" width="500"/><br>
   * In this example we have an **Endpoint Object** which is configured with multiple IP addresses. We still use a single internal DNS name and the **Service Object** that points to this **Endpoint Object** will round-robin each IP:<br>
   <img src="https://i.imgur.com/nA2XbjV.png" width="500"/><br>
 * **Sidecar pattern**
   * **Sidecars** is another type of container that runs inside our **Pods** alongside our applications.
   * They provide a connection to an external service
   * Because they run in the same **Pod** as our application they can provide access by simply connecting to **localhost / 127.0.0.1**
   * This is most useful for **proxies** such as **MySQL Proxy** for **Cloud SQL**

   ## Volumes and Persistent Storage

* Volumes
  * Container storage in itself is ephemeral. It goes away as soon as the container goes
  * We need an independent volume to define storage separate from the container
  * When we attach a volume it provides a directory mounted inside our containers so that we can access files

* Volume Types and Persistent storage
 * emptryDir<br>Scratch-space that can be shared by multiple containers in the same **Pod**. It is deleted when the **Pod** is removed from the node.

 * gcePersistentDisk<br>Native to GKE. It must be created beforehand and can be pre-populated with data. It can only be mount **read-only** by multiple consumers. Will only be unmounted when a **Pod** is removed.

 * PersistentVolumeClaim<br>Most common use. When used - the cluster makes a claim for a Persistent allocation for storage.
   * **PersistentVolume** defines a piece of storage in the cluster
   * It can be manually or dynamically provisioned using plugins
   * It can be configured with a specific **Storage Class**
   * **PersistentVolumeClaim** is a request to consume a **PersistentVolume**
   * We can then define **Access Modes** to define how a volume may be access by multiple containers.
     * **ReadWriteOnce** - a single node can mount a volume read/write
     * **ReadOnlyMany** - any node can mount the volume read-only
     * **ReadWriteMany** - any node can mount the volume read/write (not supported by GKE)<br>

   * Example **PersistentVolume**<br><img src="https://i.imgur.com/YpayXCL.png" width="500"/><br>
   * Example **PersistentVolumeClaim**<br><img src="https://i.imgur.com/wgeQ6Ma.png" width="500"/><br>
   * Example **PersistentVolumeClaim** without specified PersistentVolume<br><img src="https://i.imgur.com/I0ls8dG.png" width="500"/><br>Note: We can make a claim without providing the **PersistentVolume** - GKE will then use the default storage provider<br>

   * Example for **consuming PersistentVolumeClaim** from our **Pods**<br><img src="https://i.imgur.com/0nzhRSV.png" width="500"/>
   * Example **StorageClass**:<br><img src="https://i.imgur.com/OELbsA9.png" width="500"/><br>

   ## Config Maps & Secrets

* **Secrets**
   * Secrets are designed to store and obfuscate sensitive data (like use creds)
   * We can access secrets either by mapping to environment variable or Volume
   * By default secrets are encoded by GKE only - not encrypted (if you have access to the cluster - you have access to the code and secrets)
   * Secrets can be encrypted with Cloud KMS<br>


  * **ConfigMaps**
   * They are used to decouple configuration from images
   * If you need to make small changes to a configuration - it is not really useful to keep putting up multiple images just for small changes - so we use a sperate object - ConfigMaps
   * We create them from files, directories or literal values
   * We can insert values in ConfigMaps as environment variables
   * We can also mount ConfigMaps into Volumes and make changes without changing the image
   * Basic config map defined in yaml:<br><img src="https://i.imgur.com/pn7szxb.png" width="500"/><br>
   * The best way to consume those ConfigMaps is to define an Environment Variable for our container and set them using the ConfigMaps data:<br><img src="https://i.imgur.com/I6Amhtp.png" width="500"/><br>
   * Alternatively you can use all your key values into a container:<br><img src="https://i.imgur.com/P2O65mp.png" width="500"/><br>
   * You can also use ConfigMaps to store entire configuration files not just keys and values:<br><img src="https://i.imgur.com/zGIskVD.png" width="500"/><br>
   * And then consume them like a volume:<br><img src="https://i.imgur.com/0cutbta.png" width="500"/><br>

## **Deployment Patterns**

* Deployment Patterns gives us some really powerful options for updating Stateless Applications
* There are 3 main patterns:
 * **Rolling Updates**
   * If update the container image in our spec, GKE will gradually start replacing **Pods** with new once that have the updated spec.
   * To ensure availability we can define how many **Pods** can be created and deleted at a time so we can ensure that we have enough **Pods** to serve our traffic
   * We can also specify a threshold for failed **Pods** - which can help us determine if our update was successful or not<br><img src="https://media.giphy.com/media/gWjlDk3B9etZoUCEwl/giphy.gif" width="500"/><br>
 * **Canary deployments**
   * These allow you to test a small amount of traffic with a new version of your app to see if it safe
   * To do this in GKE we use multiple Deployment objects with a single Service object. The Service Object than can send some of the traffic to a small number of newly updated **Pods** alongside with our older production traffic to test if the **Pods** will reject or fail
   * Here is how the yaml files will look for both deployments:<br><img src="https://i.imgur.com/ohBwAXJ.gif" width="500"/><br>
   * Here's how the Canary pattern will handle the deployment:<br><img src="https://media.giphy.com/media/qoFnlGEwT79xQUHSFw/giphy.gif" width="500"/><br>
 * **Blue-Green Deployments**
   * Here we maintain two complete deployments which run on different versions
   * When we update the deployment we just update one of them and use the Service selector to point to the newly updated deployment
   * If the Deployment fail - we can just use the Service selector again to switch back to the original deployment version (much like we did in Circuit Deployment)
   * It is like flipping a switch from A to B and from B to A:<br><img src="https://media.giphy.com/media/Rb1XfqSPMY2gAMj4Xx/giphy.gif" width="500"/><br>


## **Autoscaling**

* Autoscaling is the practice of GKE automatically respond to peaks of traffic and demand
* In GKE there 3 main ways to **Autoscale**:
   * **Horizontally** scale **Pods** to increase capacity when demand is high.
    * **Auto-scales** the number of **Pod replicas** in a ReplicaSet/Deployment
    * **CPU and Memory** thresholds are observed by GKE to trigger the scaling - and when demand goes down - the **Pods** will be removed
    * You can also use **custom, multiple and external** metrics to autoscale
    * **Stakdriver** metrics can also be used
    * We use the **HorizontalPodAutoscaler** object
    * Example yaml:<br><img src="https://i.imgur.com/U6hfwvw.gif" width="500"/><br>

 * **Vertically** scale **Pods** to increase the resources assigned to individual **Pods**
   * Automatically recommends or applies **CPU and RAM** requests for **Pods**
   * More suite to **stateful** deployments
   * Cannot work alongside the **Horizontal Pod Autoscaler**
   * We use the **VerticalPodAutoscaler** object
   * Example yaml:<br><img src="https://i.imgur.com/t9p7cBq.gif" width="500"/><br>
 * **Scale cluster nodes** when demand is high so that we generate **extra VMs**
   * We use the **Node-pool cluster autoscaling** in GKE
   * Works best with **Node Pools**
   * **Node pools** are groups of VM nodes of a specific type - eg. High CPU or High RAM
