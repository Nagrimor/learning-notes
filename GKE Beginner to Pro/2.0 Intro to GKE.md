# GKE Fundamentals <img src="https://static-00.iconduck.com/assets.00/google-gke-icon-512x457-q6s0e3iu.png" alt="gke logo" width="80"/>

1. [Anatomy of a GKE cluster]()

## **Anatomy of a GKE cluster**
* Like any  other  cluster - it's a collection of computers to perform a function
* Everything in GKE is an object
* A GKE cluster contains:
 * One ore more **masters**<br>
   * They make decision (like scheduling) and provide the **control plane**
   * They have several components that define the **control plane**
   * The master is responsible for the state of the cluster. It's constantly watching everything to make sure it is as it should be
 * One or more **nodes**<br>
   * In GCP's case these are VMs.
   * They provide the runtime environment.
   * They also  provide t he resources of the cluster that can be used to run containers
 * Master:<br>
   * The **API Server** is the **front end** of the control plane. It exposes the API for all the **master** functions. Every time you or something else communicates with the **master** it will be through this API. Most of the the time you will use the **Google Cloud Console** or the **Google Command Line Tool** but in the background it is always talking to this API
   * **etcd** is the GKE's own database storing all of its configuration and state. **etcd** is a **Key Value Store** that is designed for scale and High availability
   * The **Scheduler** is responsible for scheduling workloads. This means that when you run a container - the **Scheduler** will chose a node to run it on. The node it picks can be affected by all kinds of factors such as the current load on the node, the container requirements and other customizable constraints.
   * The **Cloud Controller Manager** is what allows GKE to work with cloud platforms. It is responsible for handling things such as Networking and LoadBalancing
   * The **Kube Controller Manager** manages a handful of controllers in the cluster. It looks after objects such as nodes and other.<br>
   <img src="https://i.imgur.com/p0128YT.png" alt="gke logo" width="500"/>
 * Node:<br>
   * The first component a GKE node contains is a **kubelet**. This is a GKE agent that communicates with the **control plane** and takes instructions (such as to deploy containers when it is told to).
   * The **kube-proxy** is responsible for network connections in and out of the node
   * The **Container Runtime** runs **Docker** to allow us to run containers<br>
   <img src="https://i.imgur.com/aw61EfE.png" alt="gke logo" width="500"/>



 ## **Pods**
 * A Pod is a **logical application-centric unit** that shares network and storage configuration
 * Pods contain 1 or more tightly coupled containers
 * Example design pattern for a pod can be:
  - A **monolith** and a **web-server**
  - An **application** and a **container that proxies connection to a database** (aka database proxy)
  - An **application** and an **adapter** to process its output
  - Pods should serve a single application
 * GKE does not deploy containers on its own - puts them in pods
 * Pods are the smallest deployable units in  GKE. You cannot deploy a container on its own - you need to put it in a pod.
 * Many pods will have a single container
 * When containers run in the same pod they use the same environment. They share a local IP and can talk to each other as if they are on the same local host. They also have the same access to volumes(storage)
### **Creating pods**
* You can create pods **Dynamically** using the kubectl command in cloud shell
* The preferred way is to create pods **Declaratively**
    * This means that we create a specification file that declares to GKE the objects that we want to create
    * These specification files are usually **yaml** files
    * GKE tries to make the **live** state match the state we've declared in the configuration file
    * Example yaml file:<br>
    <img src="https://i.imgur.com/JW39gA0.png" alt="gke logo" width="500"/><br>
    * When you re-apply the same file GKE does not duplicate the object as it is already live
    * If we change the configuration GKE will update the object<br>
    <img src="https://media.giphy.com/media/eq8KmQExIuwsmiYAbV/giphy.gif" width="500"/>


## **Replica set**
* A ReplicaSet is a GKE object used to manage multiple replicas of pods
* It makes sure that all the pods are identical replicas
* A ReplicaSet  is a GKE object that provides:
  * A stable set of Pod replicas
  * A specified number of pods
  * Logic to ensure availability
* Not recommended to create ReplicaSets on their own - instead we use Deployments
* Example:<br>
We've got a replica set of 3 pods which are all the same. Those pods are split across 3 nodes to better utilize their resources. If one node drops from the cluster, the replica sets will re-create the missing pods in the other nodes to make sure they are exactly 3<br>
<img src="https://media.giphy.com/media/jq3Vxz0NUNbKBxOsJJ/giphy.gif" width="500"/>


## **Deployments**
* An **object** for logically mapping **Pods** and **ReplicaSets**
* The desired state of a **Deployment** is **enforced** by the **Controller**
* **Deployments** provide logic for **updating**, **rolling back**, and **scaling** deployments
* They also provide **error checking** for rollouts
* Deployments are defined by **yaml** files like any other object
* Example file:<br>
  <img src="https://i.imgur.com/p0dRDAe.png" width="500"/><br>
* When you update your deployments with new specification, GKE creates a new set of your deployment first. Once  the new update is up - the Deployment deletes the older version:<br>
<img src="https://media.giphy.com/media/Lp4FavzG7nRiThHqqv/giphy.gif" width="500"/>


## **Services**

* The **Service** object helps us access the constantly changing pods we've deployed using the **Deployment** and **ReplicaSet**
* The **Service** exposes a set of pods to the network
* The **Service** assigns a fixed IP to a group of pods/pod replicas so you get a stable **endpoint** without worrying about the individual IP for each pod
* There are 3 types of **Service** objects
 * **ClusterIP**<br>
 This will assign a fixed IP inside your GKE cluster. This is useful for allowing services to talk to each other internally as a ClusterIP cannot be accessed outside the cluster.
 * **NodePort**<br>
 This assigns a fixed IP to each node in your cluster. It can be useful when you are implementing a custom LoadBalancing solution
  * **LoadBalancer**<br>
  When you create this the K8 cloud controller manager will create corresponding LoadBalancer service in your corresponding Cloud Provider. In the case of GKE this is the GCP Network Load Balancer.
* **Selectors** are used to configure **Services**
 * **Selectors** are labels we add to the metadata - aka **key-value pairs**
 * **Selectors** search for **groups of labels**:
   * app=nginx
 * Pods that match Selectors become part of a Service<br>
 * In the example below we just have one selector to group our pods which is **ngingx** but we can heave multiple selectors<br>
 * Because we haven't specified which service we are creating - this yaml file will create a **ClusterIP** service by default<br>
<img src="https://i.imgur.com/sDSi6Kg.png" width="500"/><br>
* Some examples of **Services** and **Selectors**:<br>
<img src="https://i.imgur.com/WVfOogT.png" width="500"/><br>
<img src="https://i.imgur.com/pIicP1A.png" width="500"/><br>
<img src="https://i.imgur.com/VU78Jov.png" width="500"/><br>

 * Services also provide a built-in DNS name which can be resolved with anything running in your cluster
<img src="https://i.imgur.com/WFLZR83.png" width="500"/><br>

## **Pod reliability with health checks**

* **Liveness Probes**
 * A simple check performed by the kubelet service that runs on every node
 * It is something like a built-in monitoring for your pods
 * It can check an HTTP endpoint, TCP socket, or run a command
 * If the Liveness Probe fails, GKE will know something is wrong with a pod and will attempt to replace it
 * Example yaml file for a pod with a Liveness Probe:
 <img src="https://i.imgur.com/UbYl3NE.png" width="500"/><br>

* **Readiness Probe**
 * If a pod usually takes longer to boot up it is better to use the Readiness Probe
 * It is similar to the Liveness Probe and uses same methods
 * It tells GKE whether a Pod is ready to serve traffic
 * Sometimes a Pod needs longer time (for example to import data) and the Readiness Probe won't direct traffic until it's ready

* You should ideally use both probes together to tell GKE when a pods are ready and to keep a life on their health
 * Example yaml file with both probes:<br>
  <img src="https://i.imgur.com/EC3Ifzs.png" width="500"/><br>

* **Probe Configuration**
 * There are a few main ways to configure Probes
 * They are all defined by a handler in the yaml file
 * There are 3 supported types of handlers:
   * **ExecAction**<br>
   This can run a specific command inside the container and report on its exit code
   * **TCPSocketAction**<br>
   This checks for a response on a specific TCP port
   * **HTTPGetAction**<br>
   This checks a URL for a status code. (HTTP 200 code is ok - everything else is usually not)
* You can also define the initial delay and frequency of probes and also change the default seconds for timeout
  * **initalDelaySeconds**
  * **periodSeconds**
  * **timeoutSeconds**
* You can also modify the default numbers before a probe considers a check a success of fail
 * **successThreshold**
 * **failureThreshold**

* All pods start in a **Pending state** **>>>** Then hopefully they reach a **Running state** when all of the containers inside them are ready **>>>** In addition, certain types of pods are designed to execute an Action in their containers and then stop when they have entered a **Succeeded state**:<br>
<img src="https://media.giphy.com/media/KPAP9GrtfjLwqYMsdl/giphy.gif" width="500"/><br>

* If a pod does not start up or exit properly a pod enters a **Failed state** **>>>** If the Scheduler cannot determine the state of the pod it goes to **Unknown state**<br>
<img src="https://media.giphy.com/media/9gihGDRBs0fHtS9Nfx/giphy.gif" width="500"/><br>

* To ensure the health of a pod we add a **Readiness check** to tell GKE that our pod is ready to serve traffic **>>>** And we add **Liveness probe** that continually monitors the health of our pod **>>>** If the **Liveness probe** fails the pod is considered to be in a **Failed state** and GKE will try to replace it:<br>
<img src="https://media.giphy.com/media/ZfImyvoLXiuglQFN1P/giphy.gif" width="500"/><br>

## Accessing External services

* Not everything in our application stack will live inside our GKE cluster
* To access an external service we can use:
 * **Service Endpoints**:
   * They are part of GKE built-in Service Discovery System
   * We can create a Service Endpoint by defining a service yaml file with **no Selector** and a corresponding endpoint object
   * When we reference the service, GKE finds the endpoint with the corresponding name
   * Endpoints map **external services** to Service objects using an IP address or a fully qualified domain name (fqdm)
   * Because we are using Service Discovery we can access external services via internal DNS
   * Example:<br>
   <img src="https://i.imgur.com/y8bimBB.png" width="500"/><br>
   <img src="https://i.imgur.com/90UWWTP.png" width="500"/><br>
   <img src="https://i.imgur.com/4cUDijI.png" width="500"/><br>
   * We can also point to external FQDN if there is one provided and no internal endpoint objects are required. Note that we also get our internal DNS name to use with pods in this case as well:<br>
   <img src="https://i.imgur.com/b01Hdai.png" width="500"/><br>
   * In this example we have an **Endpoint Object** which is configured with multiple IP addresses. We still use a single internal DNS name and the **Service Object** that points to this **Endpoint Object** will round-robin each IP:<br>
   <img src="https://i.imgur.com/nA2XbjV.png" width="500"/><br>
 * **Sidecar pattern**
   * **Sidecars** is another type of container that runs inside our pods alongside our applications.
   * They provide a connection to an external service
   * Because they run in the same pod as our application they can provide access by simply connecting to **localhost / 127.0.0.1**
   * This is most useful for **proxies** such as **MySQL Proxy** for **Cloud SQL**
